{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network for MNIST Classification\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional networks. The dataset provides 28x28 images of handwritten digits (1 per image) and the goal is to write an algorithm that detects which digit is written. Since there are only 10 digits, this is a classification problem with 10 classes.I have implemented multilayer perceptron ANN to solve this problem. Image is already converted to flat image of size 784x1 matrix in this dataset. i have implemented this problem using tensorflow from scratch without using any API like keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-1b142094ca18>:20: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# TensorFLow includes a data provider for MNIST that we'll use.\n",
    "# This function automatically downloads the MNIST dataset to the chosen directory. \n",
    "# The dataset is already split into training, validation, and test subsets. \n",
    "# Furthermore, it preprocess it into a particularly simple and useful format.\n",
    "# Every 28x28 image is flattened into a vector of length 28x28=784, where every value\n",
    "# corresponds to the intensity of the color of the corresponding pixel.\n",
    "# The samples are grayscale (but standardized from 0 to 1), so a value close to 0 is almost white and a value close to\n",
    "# 1 is almost purely black. This representation (flattening the image row by row into\n",
    "# a vector) is slightly naive but as you'll see it works surprisingly well.\n",
    "# Since this is a classification problem, our targets are categorical.\n",
    "# With the help of one hot encoding, the target for each individual sample is a vector of length 10\n",
    "# which has nine 0s and a single 1 at the position which corresponds to the correct answer.\n",
    "# For instance, if the true answer is \"1\", the target will be [0,0,0,1,0,0,0,0,0,0] (counting from 0).\n",
    "# Have in mind that the very first time you execute this command it might take a little while to run\n",
    "# because it has to download the whole dataset. Following commands only extract it so they're faster.\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline the model\n",
    "\n",
    "The whole code is in one cell, so you can simply rerun this cell (instead of the whole notebook) and train a new model.\n",
    "The tf.reset_default_graph() function takes care of clearing the old parameters. From there on, a completely new training starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-2-57b3dec8547d>:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 5000\n",
    "\n",
    "# Reset any variables left in memory from previous runs.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#declare placeholders where the data will be fed into.\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "# Weights and biases for the first linear combination between the inputs and the first hidden layer.\n",
    "# Use get_variable in order to make use of the default TensorFlow initializer which is Xavier.\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the inputs and the first hidden layer.\n",
    "# I've chosen ReLu as our activation function.\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "# Weights and biases for the second linear combination.\n",
    "# This is between the first and second hidden layers.\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the first and the second hidden layers. Again, we use ReLu.\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    " \n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [hidden_layer_size])\n",
    "\n",
    "# Create outputs_3 variable. I'll use ReLu once again\n",
    "outputs_3 = tf.nn.relu(tf.matmul(outputs_2,weights_3) + biases_3)\n",
    "\n",
    "weights_4 = tf.get_variable(\"weights_4\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_4 = tf.get_variable(\"biases_4\", [hidden_layer_size])\n",
    "outputs_4 = tf.nn.relu(tf.matmul(outputs_3,weights_4) + biases_4)\n",
    "\n",
    "weights_5 = tf.get_variable(\"weights_5\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_5 = tf.get_variable(\"biases_5\", [hidden_layer_size])\n",
    "outputs_5 = tf.nn.relu(tf.matmul(outputs_4,weights_5) + biases_5)\n",
    "\n",
    "weights_6 = tf.get_variable(\"weights_6\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_6 = tf.get_variable(\"biases_6\", [hidden_layer_size])\n",
    "outputs_6 = tf.nn.relu(tf.matmul(outputs_5,weights_6) + biases_6)\n",
    "\n",
    "weights_7 = tf.get_variable(\"weights_7\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_7 = tf.get_variable(\"biases_7\", [hidden_layer_size])\n",
    "outputs_7 = tf.nn.relu(tf.matmul(outputs_6,weights_7) + biases_7)\n",
    "\n",
    "weights_8 = tf.get_variable(\"weights_8\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_8 = tf.get_variable(\"biases_8\", [hidden_layer_size])\n",
    "outputs_8 = tf.nn.relu(tf.matmul(outputs_7,weights_8) + biases_8)\n",
    "\n",
    "weights_9 = tf.get_variable(\"weights_9\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_9 = tf.get_variable(\"biases_9\", [hidden_layer_size])\n",
    "outputs_9 = tf.nn.relu(tf.matmul(outputs_8,weights_9) + biases_9)\n",
    "\n",
    "weights_10 = tf.get_variable(\"weights_10\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_10 = tf.get_variable(\"biases_10\", [hidden_layer_size])\n",
    "outputs_10 = tf.nn.relu(tf.matmul(outputs_9,weights_10) + biases_10)\n",
    "\n",
    "weights_11 = tf.get_variable(\"weights_11\", [hidden_layer_size, output_size])\n",
    "biases_11 = tf.get_variable(\"biases_11\", [output_size])\n",
    "\n",
    "# The outputs are a function of outputs_4, weights_5, and biases_5\n",
    "outputs = tf.matmul(outputs_10, weights_11) + biases_11\n",
    "\n",
    "\n",
    "# Calculate the loss function for every output/target pair.\n",
    "# The function used is the same as applying softmax to the last layer and then calculating cross entropy\n",
    "# This function, however, combines them in a clever way, \n",
    "# which makes it both faster and more numerically stable (when dealing with very small numbers).\n",
    "# Logits here means: unscaled probabilities (so, the outputs, before they are scaled by the softmax)\n",
    "# Naturally, the labels are the targets.\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "\n",
    "# Get the average loss\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "# I have used adam optimizer for minimizing the loss function \n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.0002).minimize(mean_loss)\n",
    "\n",
    "# Get a 0 or 1 for every input in the batch indicating whether it output the correct answer out of the 10.\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "# Batching\n",
    "batch_size = 150\n",
    "\n",
    "# Calculate the number of batches per epoch for the training set.\n",
    "batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "# Basic early stopping. Set a miximum number of epochs.\n",
    "max_epochs = 50\n",
    "\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        # Input batch and target batch are assigned values from the train dataset, given a batch size\n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # Run the optimization step and get the mean loss for this batch.\n",
    "        # Feed it with the inputs and the targets we just got from the train dataset\n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        # Increment the sum of batch losses.\n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    # So far curr_epoch_loss contained the sum of all batches inside the epoch\n",
    "    # The average batch loss is a good proxy for the current epoch loss\n",
    "    curr_epoch_loss /= batches_number\n",
    "    \n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "    # Run without the optimization step (simply forward propagate)\n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Mean loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    # Trigger early stopping if validation loss begins increasing.\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    prev_validation_loss = validation_loss\n",
    "\n",
    "print('End of training.')\n",
    "\n",
    "print(\"Training time: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy], \n",
    "    feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
